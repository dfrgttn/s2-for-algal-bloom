{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## nessisary dependents\n",
    "from glob2 import glob\n",
    "import numpy as np\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from utils import get_dataset_from_csv\n",
    "from IPython import embed\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARG_MODELS_PATH = \"models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = glob(ARG_MODELS_PATH + \"/*\")\n",
    "print(\"Founded models:\")\n",
    "for i, model in enumerate(models):\n",
    "    print(i, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARG_TEST_DATA_ORIG = \"test_reg_data_orig.csv\" \n",
    "ARG_TEST_DATA_PATH = \"test_reg_data.csv\" \n",
    "ARG_TRAIN_DATA_PATH = \"train_reg_data.csv\" \n",
    "ARG_TRAIN_DATA_ORIG = \"train_reg_data_orig.csv\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_orig = pd.read_csv(ARG_TRAIN_DATA_ORIG)\n",
    "train_data = pd.read_csv(ARG_TRAIN_DATA_PATH)\n",
    "test_data = pd.read_csv(ARG_TEST_DATA_PATH)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_HEADER = ['B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8A', 'B5_B4', \n",
    "               'B3_B2', 'B3_B4', 'B4_B5_B6', 'avw', 'B3_B5', 'diff_alg', \n",
    "               #'Area_km2', 'Shoreline_development', 'Type', \n",
    "               'chla_ug_L']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "\n",
    "eval_dataset = get_dataset_from_csv(\n",
    "    ARG_TEST_DATA_PATH, \n",
    "    CSV_HEADER,\n",
    "    \"chla_ug_L\", batch_size=batch_size\n",
    ") \n",
    "\n",
    "train_dataset = get_dataset_from_csv(\n",
    "    ARG_TRAIN_DATA_PATH, \n",
    "    CSV_HEADER,\n",
    "    \"chla_ug_L\", batch_size=batch_size\n",
    ") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "\n",
    "def MAPE(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def ReLU(x):\n",
    "    return x * (x > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure()\n",
    "lw = 2\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "model_list = []\n",
    "mse_list = []\n",
    "mse_train_list = []\n",
    "\n",
    "mae_list = []\n",
    "rmse_list = []\n",
    "r2_list = []\n",
    "msle_list = []\n",
    "mape_list = []\n",
    "\n",
    "n_param_list = []\n",
    "\n",
    "for i, model in tqdm(enumerate(models)):\n",
    "    model_name = model.split(\"/\")[1].split(\"-\")[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"eval model: {}\".format(model))\n",
    "    m0 = keras.models.load_model(model)\n",
    "    n_param = m0.count_params()\n",
    "    \n",
    "    y_pred_train = ReLU(m0.predict(train_dataset))\n",
    "    y_true_train = train_data[\"chla_ug_L\"]\n",
    "\n",
    "    mse_train = mean_squared_error(y_true_train, y_pred_train)\n",
    "    \n",
    "    \n",
    "    y_pred = ReLU(m0.predict(eval_dataset))\n",
    "    y_true = test_data[\"chla_ug_L\"]\n",
    "\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "            \n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=True)\n",
    "    \n",
    "    mape = MAPE(y_true, y_pred) / 100\n",
    "    \n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rsle = mean_squared_log_error(y_true, y_pred)\n",
    "\n",
    "    \n",
    "    model_list.append(model_name)\n",
    "    mse_list.append(mse)\n",
    "    mse_train_list.append(mse_train)\n",
    "    mae_list.append(mae)\n",
    "    rmse_list.append(rmse)\n",
    "    r2_list.append(r2)\n",
    "    msle_list.append(rsle)\n",
    "    mape_list.append(mape)\n",
    "    \n",
    "    n_param_list.append(n_param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.argmin(mse_list)\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_table = pd.DataFrame({\"Model\": model_list, \n",
    "                              \"MSE\": mse_list,\n",
    "                              \"MSE train\": mse_train_list,\n",
    "                              \"Number of params\": n_param_list,\n",
    "                            })\n",
    "summary_table = summary_table.sort_values(by=['MSE'])\n",
    "summary_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary_table.round(3).to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = pd.DataFrame({\"Model\": model_list, \n",
    "                             \"MAE\": mae_list,\n",
    "                             \"MSE\": mse_list,\n",
    "                             \"R2\": r2_list,\n",
    "                             \"MAPE\": mape_list,\n",
    "                             \"MSE train\": mse_train_list,\n",
    "                             \"Number of params\": n_param_list,\n",
    "                            })\n",
    "\n",
    "test_results.sort_values(by=['MAPE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models[89]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results.sort_values(by=['MAE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycaret\n",
    "from pycaret.regression import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_clf101 = setup(data = train_data, target = 'chla_ug_L', \n",
    "                   test_data = test_data,\n",
    "                   session_id=123) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = compare_models(n_select = 19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_list = []\n",
    "train_acc = []\n",
    "for j in range(19):\n",
    "    predictions = predict_model(best_model[j], data=test_data)\n",
    "    res = pull()\n",
    "    res_list.append(res)\n",
    "    predictions = predict_model(best_model[j], data=train_data)\n",
    "    res = pull()\n",
    "    train_acc.append(float(res[\"MSE\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pycaret= pd.concat(res_list)\n",
    "pycaret[\"MSE train\"] = train_acc\n",
    "pycaret[\"Number of params\"] = \"-\"\n",
    "pycaret = pycaret[[\"Model\", \"MAE\", \"MSE\", \"R2\", \"MSE train\", \"Number of params\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(pycaret[\"Model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.concat([test_results, pycaret])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = final.sort_values(by=['MSE'])\n",
    "final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
