{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## nessisary dependents\n",
    "from glob2 import glob\n",
    "import numpy as np\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from utils import get_dataset_from_csv\n",
    "from IPython import embed\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARG_MODELS_PATH = \"models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = glob(ARG_MODELS_PATH + \"/*\")\n",
    "print(\"Founded models:\")\n",
    "for i, model in enumerate(models):\n",
    "    print(i, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARG_TEST_DATA_ORIG = \"test_data_orig.csv\" \n",
    "ARG_TEST_DATA_PATH = \"test_data.csv\" \n",
    "ARG_TRAIN_DATA_PATH = \"train_data.csv\" \n",
    "ARG_TRAIN_DATA_ORIG = \"train_data_orig.csv\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_orig = pd.read_csv(ARG_TRAIN_DATA_ORIG)\n",
    "train_data = pd.read_csv(ARG_TRAIN_DATA_PATH)\n",
    "test_data = pd.read_csv(ARG_TEST_DATA_ORIG)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_HEADER = ['B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8A', 'B5_B4', \n",
    "               'B3_B2', 'B3_B4', 'B4_B5_B6', 'avw', 'B3_B5', 'diff_alg', \n",
    "               #'Area_km2', 'Shoreline_development', 'Type', \n",
    "               'bloom']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "\n",
    "eval_dataset = get_dataset_from_csv(\n",
    "    ARG_TEST_DATA_PATH, \n",
    "    CSV_HEADER,\n",
    "    \"bloom\", batch_size=batch_size\n",
    ") \n",
    "\n",
    "train_dataset = get_dataset_from_csv(\n",
    "    ARG_TRAIN_DATA_PATH, \n",
    "    CSV_HEADER,\n",
    "    \"bloom\", batch_size=batch_size\n",
    ") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['not-bloom', 'bloom']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import cohen_kappa_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure()\n",
    "lw = 2\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "T_lists = []\n",
    "model_list = []\n",
    "acc_list = []\n",
    "acc_train_list = []\n",
    "acc_train_orig_list = []\n",
    "\n",
    "\n",
    "auc_list = []\n",
    "recall_list = []\n",
    "pr_list = []\n",
    "f1_list = []\n",
    "kappa_list = []\n",
    "MCC_list = []\n",
    "n_param_list = []\n",
    "\n",
    "for i, model in tqdm(enumerate(models)):\n",
    "    model_name = model.split(\"/\")[1] #model.split(\"/\")[1].split(\"-\")[0]\n",
    "    \n",
    "    \n",
    "    T_list = np.linspace(0, 1, 101)\n",
    "    F1_list = []\n",
    "    gmean_list = []\n",
    "    \n",
    "    print(\"eval model: {}\".format(model))\n",
    "    m0 = keras.models.load_model(model)\n",
    "    n_param = m0.count_params()\n",
    "    \n",
    "    yhat_train = m0.predict(train_dataset)\n",
    "    y_true_train = train_data[\"bloom\"]\n",
    "    \n",
    "\n",
    "\n",
    "    for T in T_list:\n",
    "        y_pred = yhat_train >= T\n",
    "        y_true = y_true_train\n",
    "\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "        F1_list.append(f1)\n",
    "\n",
    "    T = T_list[np.argmax(F1_list)]\n",
    "    T_lists.append(T)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    y_pred_train = (yhat_train > T).astype(np.int)\n",
    "    ac_train = accuracy_score(y_true_train, y_pred_train)\n",
    "    acc_train_list.append(ac_train)\n",
    "\n",
    "\n",
    "    \n",
    "    yhat = m0.predict(eval_dataset)\n",
    "    y_true = test_data[\"bloom\"]\n",
    "    y_pred = (yhat > T).astype(np.int)\n",
    "    \n",
    "    ac = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    \n",
    "    y_pred_train = (yhat > 0.5).astype(np.int)\n",
    "    ac_train = accuracy_score(y_true, y_pred_train)\n",
    "    acc_train_orig_list.append(ac_train)\n",
    "    \n",
    "    \n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_true, yhat)\n",
    "        \n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    rec = recall_score(y_true, y_pred)\n",
    "    pre = precision_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    kappa = cohen_kappa_score(y_true, y_pred)\n",
    "    mc = matthews_corrcoef(y_true, y_pred)\n",
    "    \n",
    "    model_list.append(model_name)\n",
    "    acc_list.append(ac)\n",
    "    \n",
    "    auc_list.append(auc)\n",
    "    recall_list.append(rec)\n",
    "    pr_list.append(pre)\n",
    "    f1_list.append(f1)\n",
    "    kappa_list.append(kappa)\n",
    "    MCC_list.append(mc)\n",
    "    n_param_list.append(n_param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.split(\"/\")[1]#.split(\"-\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.argmax(acc_list)\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_table = pd.DataFrame({\"Model\": model_list, \n",
    "                              \"T\": T_lists,\n",
    "                              \"Accuracy\": acc_list,\n",
    "                              \"Accuracy train\": acc_train_list,\n",
    "                              \"Accuracy orig\": acc_train_orig_list,\n",
    "                              \"Number of params\": n_param_list,\n",
    "                            })\n",
    "summary_table = summary_table.sort_values(by=['Accuracy'], ascending=False)\n",
    "summary_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary_table.round(3).to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = pd.DataFrame({\"Model\": model_list, \n",
    "                             \"Accuracy\": acc_list,\n",
    "                             \"AUC\":auc_list,\n",
    "                             \"Recall\":recall_list,\n",
    "                             \"Prec.\":pr_list,\n",
    "                             \"F1\":f1_list,\n",
    "                             \"Kappa\":kappa_list,\n",
    "                             \"MCC\":MCC_list,\n",
    "                             #\"TT (Sec)\":0.0\n",
    "                             \"T\": T_lists,\n",
    "                             \"Accuracy (train)\": acc_train_list,\n",
    "                             \"Number of params\": n_param_list,\n",
    "                            })\n",
    "\n",
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = test_results.sort_values(by=['Accuracy'], ascending=False)\n",
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = test_results.sort_values(by=['AUC'], ascending=False)\n",
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_results = test_results.sort_values(by=['Prec.'], ascending=False)\n",
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycaret\n",
    "from pycaret.classification import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_clf101 = setup(data = train_data, target = 'bloom', \n",
    "                   test_data = test_data,\n",
    "                   remove_multicollinearity = True, multicollinearity_threshold = 0.4,\n",
    "                   feature_selection = True,\n",
    "                   feature_interaction = True,\n",
    "                   feature_ratio = True,\n",
    "                   session_id=123) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = compare_models(n_select = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_model(best_model, 'best_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_list = []\n",
    "train_acc = []\n",
    "for j in range(15):\n",
    "    predictions = predict_model(best_model[j], data=test_data)\n",
    "    res = pull()\n",
    "    res_list.append(res)\n",
    "    predictions = predict_model(best_model[j], data=train_data)\n",
    "    res = pull()\n",
    "    train_acc.append(float(res[\"Accuracy\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pycaret= pd.concat(res_list)\n",
    "pycaret[\"Accuracy (train)\"] = train_acc\n",
    "pycaret[\"Number of params\"] = \"-\"\n",
    "pycaret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(pycaret[\"Model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.concat([test_results, pycaret])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = final.sort_values(by=['Accuracy'], ascending=False)\n",
    "final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
